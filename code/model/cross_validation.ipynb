{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKhFzZq5O0GT"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers transformers==4.40.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the required packages\n",
        "\n",
        "# Dataframes\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# Regular expressions\n",
        "import re\n",
        "\n",
        "# Unidecoder\n",
        "import unicodedata\n",
        "\n",
        "# Timestamp / time measurment\n",
        "import time\n",
        "\n",
        "# for train/test data preparation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Label encode\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Class weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Model performance scores\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "# PyTorch: enable GPU access\n",
        "import torch\n",
        "\n",
        "# Simpletransformers classifier\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n"
      ],
      "metadata": {
        "id": "mmV1FmmHO7RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "Code for cross-validation inspired by Hauke Licht's Cross-lingual supervised text classification tutorial: https://github.com/haukelicht/crosslingual-supervised-text-classification-tuorial?tab=readme-ov-file"
      ],
      "metadata": {
        "id": "AOjFoKewPFqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "dat = pd.read_csv('training_data.csv')\n",
        "\n",
        "dat['final_climate']=dat['final_climate'].astype(int)\n",
        "dat['final_climate'].sum()"
      ],
      "metadata": {
        "id": "0cMAAiOhO-mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Â set qs_id as index b\n",
        "dat.set_index(\"qs_new\", drop = False, inplace = True, verify_integrity = True)"
      ],
      "metadata": {
        "id": "rtqTTTdIO-yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make numeric labels\n",
        "dat[\"label\"] = dat[\"final_climate\"].astype(\"category\").cat.codes\n",
        "dat[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "Ry1j2tGxO-6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make stratifications of data by langauge and climate relevance, from https://stackoverflow.com/a/62918682\n",
        "dat[\"strata_\"] = dat.set_index(['language','label']).index.factorize()[0]"
      ],
      "metadata": {
        "id": "KgL5_tUrO_Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up GPU"
      ],
      "metadata": {
        "id": "1FIdrNc2QE-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to select a specific GPU, set it here:\n",
        "# gpu = 0\n",
        "# torch.cuda.set_device(gpu)\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use GPU {}:'.format(torch.cuda.current_device()), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "IhYb_UEgO_G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up for cross validation"
      ],
      "metadata": {
        "id": "qo_7eFTBQOBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE [train+test]/validation split (80-20)\n",
        "\n",
        "train_test_ids, val_ids = train_test_split(dat.index.values, test_size = .2, stratify = dat.strata_.values)\n",
        "\n",
        "print(len(train_test_ids), \"training samples\")\n",
        "print(len(val_ids), \"val samples\")\n"
      ],
      "metadata": {
        "id": "4tEtMpnMNwPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train, val, test dfs\n",
        "train_test_df = pd.DataFrame(zip(dat.loc[train_test_ids]['original_text'].values,dat.loc[train_test_ids][f'final_climate'].values,dat.loc[train_test_ids]['strata_'].values),columns=['text','labels','strata_'])\n",
        "val_df = pd.DataFrame(zip(dat.loc[val_ids]['original_text'].values,dat.loc[val_ids][f'final_climate'].values),columns=['text','labels'])"
      ],
      "metadata": {
        "id": "_nqlLrZXODjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the labels\n",
        "train_test_df['labels'] = label_encoder.fit_transform(train_test_df.labels)\n",
        "val_df['labels'] = label_encoder.fit_transform(val_df.labels)"
      ],
      "metadata": {
        "id": "JFYSpimhOT4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_df.to_csv(f'cv_train_test.csv',header=True,index=False)\n",
        "val_df.to_csv(f'cv_val.csv',header=True,index=False)"
      ],
      "metadata": {
        "id": "yLS8sKBDOVIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKE THE FOLDS\n",
        "N_FOLDS = 5\n",
        "cv_idxs = np.random.choice(range(N_FOLDS), len(train_test_ids))\n",
        "\n",
        "cv_folds_ids = list()\n",
        "\n",
        "for fold in range(5):\n",
        "  idxs = cv_idxs == fold\n",
        "  # train IDs, test IDs\n",
        "  cv_folds_ids.append( ( train_test_ids[np.logical_not(idxs)], train_test_ids[idxs] ) )\n"
      ],
      "metadata": {
        "id": "-cAAcj_BQp27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ARRANGE INTO DICTIONARY\n",
        "cv_folds_dict = dict()\n",
        "for n,_ in enumerate(cv_folds_ids):\n",
        "  cv_folds_dict[n]=cv_folds_ids[n]"
      ],
      "metadata": {
        "id": "h0Xl3NFNNf6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up model arguments\n",
        "\n",
        "model_args = ClassificationArgs()\n",
        "model_args.reprocess_input_data = True\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.evaluate_during_training = True\n",
        "model_args.manual_seed = 4\n",
        "model_args.use_multiprocessing = True\n",
        "\n",
        "model_args.train_batch_size = 8\n",
        "model_args.eval_batch_size = 8\n",
        "model_args.num_train_epochs = 2\n",
        "model_args.learning_rate= 1e-5\n",
        "model_args.max_seq_length = 256\n",
        "model_args.do_lower_case=True\n",
        "model_args.sliding_window = True\n",
        "model_args.stride = 0.6\n",
        "\n",
        "model_args.weight_decay = .187677\n",
        "model_args.hidden_dropout_prob =  .188775\n",
        "model_args.attention_probs_dropout_prob = .330174\n",
        "\n",
        "model_args.labels_list = [0,1]\n",
        "model_args.no_save = True\n",
        "model_args.save_model_every_epoch=False\n",
        "model_args.save_optimizer_and_scheduler=False\n"
      ],
      "metadata": {
        "id": "mQhK1EPeO_Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_dict = dict()\n",
        "preds_dict = dict()\n",
        "\n",
        "for n,fold in cv_folds_dict.items():\n",
        "  # create train, test dfs\n",
        "  train_df = pd.DataFrame(zip(dat.loc[fold[0]]['original_text'].values,dat.loc[fold[0]]['label'].values),columns=['text','labels'])\n",
        "  test_df = pd.DataFrame(zip(dat.loc[fold[1]]['original_text'].values,dat.loc[fold[1]]['label'].values),columns=['text','labels'])\n",
        "\n",
        "  # Examine size of splits\n",
        "  print(train_df.shape[0], test_df.shape[0])\n",
        "\n",
        "  # TEST STRATA DISTRIBUTIONS ACROSS FOLDS\n",
        "  # train_dist = df.loc[fold[0]].groupby([\"language\", \"label\"]).size().unstack()\n",
        "  # test_dist  = df.loc[fold[1]].groupby([\"language\", \"label\"]).size().unstack()\n",
        "\n",
        "  # Load the label encoder\n",
        "  label_encoder = LabelEncoder()\n",
        "\n",
        "  # Encode the labels\n",
        "  train_df['labels'] = label_encoder.fit_transform(train_df.labels)\n",
        "  test_df['labels'] = label_encoder.fit_transform(test_df.labels)\n",
        "\n",
        "\n",
        "  print(f'Fold: {n}')\n",
        "\n",
        "  weights = compute_class_weight(class_weight = 'balanced', classes=[0,1], y=train_df.labels)\n",
        "  weights = [*weights]\n",
        "\n",
        "  model = ClassificationModel(model_type, model_name,  weight=weights, num_labels=2, args=model_args)\n",
        "  print(model.train_model(train_df, eval_df = test_df))\n",
        "\n",
        "  eval_result, eval_model_outputs, eval_wrong_predictions = model.eval_model(val_df,\n",
        "                                                              f1_score = f1_class,\n",
        "                                                              acc=accuracy_score,\n",
        "                                                              recall=recall,\n",
        "                                                              precision=precision)\n",
        "\n",
        "\n",
        "  print(eval_result)\n",
        "  results_dict[n]=eval_result\n",
        "\n",
        "  preds,output = model.predict(val_df['text'].tolist())\n",
        "  true_labels = val_df['labels']\n",
        "  print(classification_report(true_labels, preds))\n",
        "\n",
        "  val_df['preds']=preds\n",
        "  val_df['original_text']=val_df['text']\n",
        "  preds_merged = df.merge(val_df,on='original_text',how='inner')\n",
        "  preds_dict[n]=preds_merged\n"
      ],
      "metadata": {
        "id": "mq60Bq0HNjAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save overall results\n",
        "pd.DataFrame.from_dict(results_dict).to_csv('results_5-fold_cv.csv')"
      ],
      "metadata": {
        "id": "PWv9DhSYPIFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_perf=pd.DataFrame()\n",
        "# language specific f1 score\n",
        "for k in preds_dict.keys():\n",
        "  lang_fold_perf = dict()\n",
        "  for i in preds_dict[k]['language'].unique():\n",
        "    s = preds_dict[k][preds_dict[k]['language']==i]\n",
        "    try:\n",
        "      lang_fold_perf[i]=classification_report(s['labels'], s['preds'],output_dict=True)['1']['f1-score']\n",
        "    except:\n",
        "      print('Error with', i)\n",
        "  out = pd.DataFrame(lang_fold_perf.values(),index=lang_fold_perf.keys()).T\n",
        "  lang_perf=pd.concat([lang_perf,out])\n"
      ],
      "metadata": {
        "id": "ise228RhQznT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save per language results\n",
        "lang_perf.to_csv('language_performance.csv')"
      ],
      "metadata": {
        "id": "QEzl43woRLy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}